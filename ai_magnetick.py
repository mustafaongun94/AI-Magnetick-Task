# -*- coding: utf-8 -*-
"""AI Magnetick

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nAsL9pp58iOfBgF5GnOGHATkg7GUvee_
"""

import pandas as pd
import numpy as np
import mplfinance as mpf
from datetime import datetime
from sklearn.preprocessing import LabelEncoder
from xgboost import XGBRegressor

"""**Q1 - Please load the csv file to your notebook.**"""

data = pd.read_csv("/sample_date_for_challenge.csv",index_col=0,parse_dates=True)

"""**Q2 - Please plot candle sticks along with the vwap_est as seen in Figure 1 below for 2023-05-24 between 9:30 am and 11 am for 5 min intervals. An approximate graph works, do not lose time on details.**"""

data['datetime_est_'] = data['date_est'].str.cat(data['time_est'], sep=' ')
data_est = data[['datetime_est_','open','high','low','close','volume','vwap_est']]
start_time = datetime.strptime("2023-05-24 09:30:00-00:00", "%Y-%m-%d %H:%M:%S%z")
end_time = datetime.strptime("2023-05-24 11:00:00-00:00", "%Y-%m-%d %H:%M:%S%z")
data_est['datetime_est_'] = pd.to_datetime(data_est['datetime_est_'])
data_est.index = pd.DatetimeIndex(data_est['datetime_est_'])

data_example = data_est.loc[start_time:end_time]
data_example = data_example.drop(columns='datetime_est_')
data_example

data_example_5t = data_example.resample("5T").mean()
mpf.plot(data_example_5t,type='candle',style='yahoo',volume=True,addplot=[mpf.make_addplot(data_example_5t['vwap_est'])])

"""**Q3- Could you add lagged features of price and volume from lag-1 minute to lag-5 minutes by using the relevant methods in pandas?**"""

for i in range(1, 6):
    data[f'vwap_lag{i}'] = data['vwap'].shift(i)
for i in range(1, 6):
    data[f'volume_lag{i}'] = data['volume'].shift(i)
data.dropna(inplace=True)
data

"""Data Preprocessing for Q-4"""

data['datetime'] = pd.to_datetime(data['datetime'])
data.index = pd.DatetimeIndex(data['datetime'])
data = data.drop(columns=['datetime_est_','datetime_est','date_est','datetime','time_est','vwap_est'])

label_encoder = LabelEncoder()
data['break_bounce'] = label_encoder.fit_transform(data['break_bounce'])
data.head(50)
one_hot_encoded = pd.get_dummies(data['trend'], prefix='trend')
data = pd.concat([data, one_hot_encoded], axis=1)
data.drop(columns=['trend'], axis=1, inplace=True)

"""Train and test split for time series analysis"""

train = data.loc[:"2023-05-02"]
test = data.loc["2023-05-02":]
y_train = train['vwap']
x_train = train.drop(columns='vwap')
y_test = test['vwap']
x_test = test.drop(columns='vwap')

"""**Q4- Could you write a Random Forest or XGBoost Model with the aim of predicting the current price using the lagged features that you defined in Q3?**"""

model = XGBRegressor()

model.fit(x_train,y_train)
y_predict = model.predict(x_test)

"""**Q5-  Could you report the performance of your model in your test set (which could be the last one month of observations)?  Please explain the reason behind the metric that you choose for reporting the performance.**"""

def mean_absolute_percentage_error(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100
mape = mean_absolute_percentage_error(y_test,y_predict)
mape

"""I thought it would be more appropriate to use the MAPE error metric, as MAE and MSE are sensitive to large error values. MAPE expresses the error rate as a percentage and is resistant to large error values."""